<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">How to create Quarkus Command Mode applications</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rZKO1Wc44K0/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/how-to-create-quarkus-command-mode-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-create-quarkus-command-mode-applications</id><updated>2021-10-18T09:36:57Z</updated><content type="html">Quarkus is set of technologies to develop an entire Microservice architecture. The foundation of this architecture is typically an HTTP server, serving REST Endpoints. It is however also possible to create powerful Java scripts using Quarkus advanced sets of APIs. In this tutorial we will learn how to create standalone Quarkus applications with a bare ... The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rZKO1Wc44K0" height="1" width="1" alt=""/&gt;</content><dc:creator>F.Marchioni</dc:creator><feedburner:origLink>http://www.mastertheboss.com/soa-cloud/quarkus/how-to-create-quarkus-command-mode-applications/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=how-to-create-quarkus-command-mode-applications</feedburner:origLink></entry><entry><title>Announcing Cryostat 2.0: JDK Flight Recorder for containers</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Pm6JlnaXKVk/announcing-cryostat-20-jdk-flight-recorder-containers" /><author><name>Andrew Azores, Janelle Law</name></author><id>492172b0-3388-4006-aae4-c41b855e3698</id><updated>2021-10-18T07:00:00Z</updated><published>2021-10-18T07:00:00Z</published><summary type="html">&lt;p&gt;Cryostat is a container-native JVM application that provides a secure API for profiling and monitoring &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; with &lt;a href="https://developers.redhat.com/blog/2020/08/25/get-started-with-jdk-flight-recorder-in-openjdk-8u"&gt;JDK Flight Recorder&lt;/a&gt; (JFR). JDK Flight Recorder collects profiling and diagnostic data from applications using JFR events stored in binary flight recordings. When requested, Cryostat can retrieve, store, and analyze flight recordings from containerized Java virtual machines (JVMs) to assess overall application health. Users can download recording files and upload them to &lt;a href="https://www.oracle.com/java/technologies/jdk-mission-control.html"&gt;JDK Mission Control&lt;/a&gt; (JMC) or &lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt; for further analysis.&lt;/p&gt; &lt;p&gt;This article introduces Cryostat and shares new features in the 2.0 release, including example use cases, tips for getting started, and additional release notes. For more information about Cryostat fundamentals, visit &lt;a href="https://developers.redhat.com/blog/2021/01/25/introduction-to-containerjfr-jdk-flight-recorder-for-containers#"&gt;Introduction to Cryostat: JDK Flight Recorder for containers&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Cryostat 2.0 is currently in tech preview for &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; and other Kubernetes platforms.&lt;/p&gt; &lt;h2&gt;Using Cryostat with JDK Flight Recorder&lt;/h2&gt; &lt;p&gt;Cryostat is compatible with JVMs that (1) can accept incoming JMX connections, and (2) include JDK Flight Recorder. This includes OpenJDK and most downstream builds or distributions of OpenJDK, such as &lt;a href="https://developers.redhat.com/products/openjdk/download"&gt;Red Hat OpenJDK&lt;/a&gt; and &lt;a href="https://adoptium.net/"&gt;Adoptium OpenJDK&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The JDK Mission Control desktop-only application requires users to retrieve flight recordings on a local disk. When bare-metal JDK Flight Recorder writes recordings to the local file system inside a container, those recordings are lost when the container restarts. A complicated and likely insecure workaround would involve establishing a direct connection between a desktop workstation and the containerized JVMs in the cloud over Java Management Extensions (JMX).&lt;/p&gt; &lt;p&gt;Cryostat securely retrieves and stores flight recordings from containerized JVMs, so users can access these recordings over HTTP/HTTPS. Cryostat’s low overhead during both profiling and continuous monitoring allows users to monitor application performance in production. Users can start recordings with built-in or custom &lt;a href="https://github.com/cryostatio/cryostat#event-templates"&gt;event templates&lt;/a&gt; in multiple JVMs at once.&lt;/p&gt; &lt;h2&gt;Use cases for Cryostat 2.0&lt;/h2&gt; &lt;p&gt;Example use cases for Cryostat 2.0 include automated rules, batch retrieval of JDK Flight Recorder data, and custom target definitions and support for non-RMI JMX connectors. We'll write more about each of these use cases in future articles.&lt;/p&gt; &lt;h2&gt;Highlights of the 2.0 release&lt;/h2&gt; &lt;p&gt;We've added the following major new features in Cryostat 2.0:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Cryostat Operator: Cryostat is now offered as an OpenShift Operator, also in tech preview, available from the OperatorHub on OpenShift 4.6 and higher. The Cryostat Operator eases configuration and deployment of Cryostat into your OpenShift namespaces and provides an OpenShift custom resource interface to the Cryostat API, enabling you to build more integrations with other OpenShift Operators and services in more flexible ways.&lt;/li&gt; &lt;li&gt;Automated rules: A common use case for Cryostat is to enable continuous monitoring, or other JFR event templates, to all or some subset of JVMs in the target application. Automated rule definitions allow users to specify what kinds of JVMs should be triggered on—using criteria like port number, OpenShift annotations, OpenShift labels, or JVM alias—along with what kind of JDK Flight Recorder events should be recorded, and how often the JFR data should be copied from each target into the Cryostat archive.&lt;/li&gt; &lt;li&gt;Batch operations: Another common use case for Cryostat is to retrieve all JFR data from a set of target JVMs when some external event occurs. In Cryostat 1.0, a client would need to observe the external event and then fire &lt;em&gt;N&lt;/em&gt; HTTP requests to Cryostat to retrieve JFR data from each of the &lt;em&gt;N&lt;/em&gt; target applications. In Cryostat 2.0, a shorthand allows clients to fire one HTTP request and prompt Cryostat to retrieve data from a set of targets into the Cryostat archives. The client can then download the JFR data from Cryostat as needed.&lt;/li&gt; &lt;li&gt;Custom targets: Users can now create target definitions manually, rather than relying upon Cryostat's platform-tailored JVM discovery mechanisms.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Keep an eye out for upcoming articles highlighting and demonstrating these features.&lt;/p&gt; &lt;h2&gt;How to get started with Cryostat 2.0&lt;/h2&gt; &lt;p&gt;Install the Cryostat Operator to deploy Cryostat on OpenShift or another Kubernetes cluster. Check out the following links for installation and usage instructions:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The recommended installation guide at &lt;a href="https://cryostat.io"&gt;cryostat.io&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Our guide to &lt;a href="https://developers.redhat.com/blog/2021/01/25/introduction-to-containerjfr-jdk-flight-recorder-for-containers#manual_cryostat_installation_and_setup"&gt;manual Cryostat installation and setup&lt;/a&gt; for testing and demonstration purposes.&lt;/li&gt; &lt;li&gt;&lt;a href="https://cryostat.io/getting-started/"&gt;Getting started with the Cryostat web UI&lt;/a&gt; at cryostat.io.&lt;/li&gt; &lt;li&gt;The Cryostat API usage guide: &lt;a href="https://github.com/cryostatio/cryostat/tree/v2/HTTP_API.md"&gt;HTTP_API.md&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;The &lt;a href="https://github.com/cryostatio/cryostat/tree/v2/README.md"&gt;Cryostat README&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Additional 2.0 features&lt;/h2&gt; &lt;p&gt;There are a few other minor new features in Cryostat 2.0:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/cryostatio/cryostat#jmx-connectors"&gt;Client-defined connection protocols&lt;/a&gt;: Allow Cryostat to communicate with target JVMs using different JMX protocol implementations (see &lt;a href="https://github.com/cryostatio/cryostat/pull/482"&gt;pull request #482&lt;/a&gt; for details).&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/cryostatio/cryostat/blob/main/HTTP_API.md#certificateposthandler"&gt;POST /api/v2/certificates&lt;/a&gt;: Upload an SSL certificate to trust when connecting to target JVMs.&lt;/li&gt; &lt;li&gt;&lt;code&gt;GET /api&lt;/code&gt;: List all API operations.&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/cryostatio/cryostat/blob/main/HTTP_API.md#notificationsurlgethandler"&gt;Push notifications for Cryostat operations&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Breaking changes since Cryostat 1.0&lt;/h2&gt; &lt;p&gt;The Cryostat 2.0 release brings some breaking changes since version 1.0. We felt these changes were necessary as Cryostat matures into a full-fledged product that can best serve our users’ diverse needs.&lt;/p&gt; &lt;h3&gt;Removal of the interactive command channel&lt;/h3&gt; &lt;p&gt;The interactive command channel has been replaced with a one-way WebSocket notifications channel. Cryostat was initially implemented as an interactive-mode utility with user actions limited to a set of pre-defined commands. This functionality was later extended to allow remote network access using WebSocket, resulting in the command channel. However, now that all of the command channel functionality has been ported over to the HTTP API, the channel has been adapted to instead provide one-way notifications to any targets connected to the Cryostat instance.&lt;/p&gt; &lt;h3&gt;Removal of event specifier strings&lt;/h3&gt; &lt;p&gt;Previously, users were able to specify JDK Flight Recorder events to be monitored during a recording using event specifier string syntax (such as &lt;code&gt;jdk.SomeEvent:optionName=value;jdk.AnotherEvent:secondOption=value2;...)&lt;/code&gt;. Now, you can specify events during recording creation by providing the JDK Flight Recorder event template name. Users can pick from a variety of standard templates or upload their own custom templates to Cryostat. Because they consist of XML, templates are easier to create, edit, validate, and share, and are also interoperable, as they are the standard JDK format.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The tech preview release of Cryostat 2.0 lets you monitor containerized JVMs on an OpenShift or Kubernetes cluster. Furthermore, you can create, manage, and download JDK Flight Recorder recordings for analysis in &lt;a href="https://www.oracle.com/java/technologies/jdk-mission-control.html"&gt;JDK Mission Control&lt;/a&gt; or &lt;a href="https://grafana.com/"&gt;Grafana&lt;/a&gt;. To learn more about Cryostat 2.0, visit the Cryostat homepage, &lt;a href="https://cryostat.io"&gt;cryostat.io&lt;/a&gt;, and feel free to join our &lt;a href="https://groups.google.com/g/cryostat-development"&gt;mailing list&lt;/a&gt; to connect to the upstream community.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/18/announcing-cryostat-20-jdk-flight-recorder-containers" title="Announcing Cryostat 2.0: JDK Flight Recorder for containers"&gt;Announcing Cryostat 2.0: JDK Flight Recorder for containers&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Pm6JlnaXKVk" height="1" width="1" alt=""/&gt;</summary><dc:creator>Andrew Azores, Janelle Law</dc:creator><dc:date>2021-10-18T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/18/announcing-cryostat-20-jdk-flight-recorder-containers</feedburner:origLink></entry><entry><title type="html">New Keycloak maintainer: Takashi Norimatsu</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/BCeRmPPPKHE/takashi.adoc.html" /><author><name>Stian Thorgersen</name></author><id>https://www.keycloak.org//2021/10/takashi.adoc.html</id><updated>2021-10-18T00:00:00Z</updated><content type="html">We are extremely pleased to welcome as an official maintainer of Keycloak. Takashi has contributed to Keycloak since 2017, with a focus on security features of OAuth 2.0 and OpenID Connect, such as PKCE, strong signature algorithms, and Certificate Bound Access Tokens. More recently, he has been leading development related to Financial-grade API (FAPI) in the FAPI special interest group. In addition he has been helping other developers in the area of API authorization, including giving presentations at multiple conferences. Takashi will continue leading development of OAuth 2.0 and OpenID Connect security related features, with an initial focus on features needed to provide higher level of API security for enterprise scenarios. Takashi works for Hitachi, Ltd. in Japan, which sees the real value of Keycloak especially in the API management area, allowing him to invest a significant portion of his time to the Keycloak project. The Keycloak team is very exited about having Takashi join us as a maintainer, and we are looking forward to working more closely with Takashi going forward.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/BCeRmPPPKHE" height="1" width="1" alt=""/&gt;</content><dc:creator>Stian Thorgersen</dc:creator><feedburner:origLink>https://www.keycloak.org//2021/10/takashi.adoc.html</feedburner:origLink></entry><entry><title type="html">Event Driven Drools: CEP (Complex Event Processing) Explained</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/LS5Vilh35h0/event-driven-drools-cep-complex-event-processing-explained.html" /><author><name>Luca Molteni</name></author><id>https://blog.kie.org/2021/10/event-driven-drools-cep-complex-event-processing-explained.html</id><updated>2021-10-14T10:32:32Z</updated><content type="html">In this article we’ll introduce you to a powerful feature of Drools, CEP (Complex Event Processing) To understand the context, let me introduce the idea about real time computing, a specific branch of computing in which the response of the system is a key component of the result. In other terms, in a real-time system we care not only about the result, but also when we receive it. As an example, if a big company is creating a report out of a database we probably don’t care about how long it takes to process it. Most probably management wants to have it as soon as possible, but most of the time, they won’t care if it’s delivered in thirty minutes or in one hour. Let’s imagine that you’re travelling by plane to your destination, and suddenly one of the engines of the airplane stops working. The pilot wants to be alerted as soon as possible to take the eventual steps to land the airplane safely. In this case time is an essential factor of the computation. If the information is delivered late, the safety of the people can be put at risk. It’s important that the software of the airplane runs on a real time system, so that we’re sure that the information comes instantly. In this example, we call the information on the engine stopping an event. WHAT IS AN EVENT? After the examples, you might have a general idea of what an event is: it’s something that happens in a specific moment in time and that changes the state of the system significantly. Event processing is then a set of tools that let developers write programs against those facts to process them and act accordingly. For example, we might react to the fact that the price of a Bitcoin is currently less than 2000$ and it could be a good idea to buy it. Definitely not as important as an airplane having a problem, but it might be useful if you’re into cryptocurrencies trading. Drools is mainly a rule engine so not an end-to-end real time solution, it is not intended to provide any guarantees of delivery time similar to ad-hoc trading systems. But it does execute a lot of rules very fast and moreover it does provide tools to process events by writing rules in the same language we use to write business rules: DRL. Finally, together with Kogito it becomes an end to end cloud-native business automation solution that can react to domain specific events. WHAT IS CEP? Complex event processing is an evolution of event processing allowing it to react to temporal correlations among events. The system, in this case Drools, provides tools to make it easy for the users to mix the information available by creating extractions and projections. CEP systems have been out for a while, but it’s an always actual topic. For example Reactive systems use events to promote decoupling between various parts of your architecture and can ease the maintenance. In this example we have three different microservices tightly coupled together. One microservice calls directly the other, and inserting a new service in between might be complicated. Another problem with this architecture is handling distributed transactions. When introducing some kind of event processing to orchestrate microservices we decouple the services from one another and adding a new service will be a matter of creating new types of events and handling them. Events will also inform the services of eventual problems or errors in the process so that each service can rollback its transaction independently, leading to a more robust distributed transaction mechanism. Also in modern reactive Java development we saw some example of Complex Event Processing, if we take a look at Vert.X or its Camel Bridge () we might see that idea of passing Events is ubiquitous. WHAT IS CEP USEFUL FOR? There are many use cases for CEP, some of them might be * Stock Market * IOT * Fraud Detection * Monitoring In this article we’re going to take a look at an example of monitoring. You can find the source code of the example on EXAMPLE A MONITORING SERVICE WRITTEN WITH DROOLS The example provided is targeting Drools 7.59.0.Final, while we expect that most of the CEP related concepts will be identical in Kogito, the syntax of the rule and the API will probably differ slightly. First of all we have to understand what an event is for Drools. An event is a fact like every other object inserted inside a Drools working memory with some kind of temporal information. If we provide a single timestamp the event is a Punctual Event, for example an alarm. There is another type of events which are called Interval events that have a begin timestamp and a duration. In our monitoring service, we have a heartbeat event that is expected to signal that our monitored system is alive. All we need is a few annotations on Java POJOs. There are other ways to define an event, please refer to . @Role(Role.Type.EVENT) @Timestamp("ts") public class HeartBeat { private Date ts; public HeartBeat() { } public Date getTs() { return ts; } public void setTs(Date ts) { this.ts = ts; } } To enable CEP processing in Drools we must configure the KieBase to work in the STREAM mode. To do so, in Drools we can configure it via the kmodule.xml file like this: &lt;kmodule xmlns="; &lt;kbase name="CEPExplained" eventProcessingMode="stream" packages="kie.live"&gt; &lt;/kbase&gt; &lt;/kmodule&gt; Let’s now write the first rule, we want to check whether the heartbeat event is coming every five seconds. If it’s late for any reason, we want to communicate it, in this case using a System.out.println but surely in a real monitoring system it would send a notification of some kind. We’ll also add the last heartbeat to a collection so that we can check the last event inserted. rule "heartbeat rule" when $h : HeartBeat( $ts: ts) not(HeartBeat( this != $h, this after[0s, 5s] $h)) then System.out.println("Heartbeat not received in 5s after: " + $ts); controlSet.push($h); // clear this in production end Then we can write a test testing this functionality. Deque check = new ArrayDeque&lt;&gt;(); session.setGlobal("controlSet", check); HeartBeat hb1 = new HeartBeat(); Date hb1Date = Date.from(Instant.now()); hb1.setTs(hb1Date); session.insert(hb1); // You shouldn't probably test like this Thread.sleep(6000); session.fireAllRules(); assertEquals(hb1Date, check.pop().getTs()); This test looks strange: there’s a Thread.sleep inside. This means that your test will stay idle for six seconds without doing anything. If you write a lot of tests like this, the whole suite will slow down significantly for no reason. Drools provides a better tool to deal with time testing, called Pseudo Clock. Pseudo Clock is a mechanism to have a kind of simulated clock, totally unrelated to the actual computer clock, that we can handle accordingly. This is the previous test written with the Pseudo Clock: KieSessionConfiguration conf=KieServices.Factory.get().newKieSessionConfiguration(); conf.setOption(ClockTypeOption.PSEUDO); KieSession session=kieBase.newKieSession(conf,null); ... SessionPseudoClock clock=session.getSessionClock(); HeartBeat hb1=new HeartBeat(); Date hb1Date=Date.from(Instant.ofEpochMilli(clock.getCurrentTime())); hb1.setTs(hb1Date); session.insert(hb1); clock.advanceTime(5,TimeUnit.SECONDS); session.fireAllRules(); assertEquals(hb1,check.pop()); The first lines are to configure the pseudo clock programmatically only in this test, it’s also possible to do it in the kmodule.xml like this. &lt;kbase name="CEPExplained" eventProcessingMode="stream" packages="kie.live" &gt; &lt;ksession name="default" clockType="pseudo" /&gt; &lt;/kbase&gt; The rest of the test looks similar, but instead of calling the Java Instant.now we ask to the Pseudo Clock for the timestamp with clock.getCurrentTime() and instead of waiting with Thread.sleep we use clock.advanceTime(5, TimeUnit.SECONDS). If you try running this test from the example you’ll see that it runs very fast, without test interruptions. COMPLEX EVENT PROCESSING The previous basic monitoring scenario is basic event processing. Let’s try and promote it to Complex Event Processing, and by that I mean to correlate various types of events. For example our monitoring system might decide to notify some external service of how many time a computer has been rebooted. We’ll have another kind of event called SystemRebootEvent that will have a timestamp and a duration of the reboot, and we can count the events with an accumulate function. The SystemRebootEvent will also have a userId field that we’ll use to get the information about the system administrator of the computer from the Drools memory, exactly how we do with a normal rule with another pattern, by creating a new User pattern with a condition. rule "Computer is rebooting too many times" when $r1 : SystemRebootEvent($cId : computerId, $uId: userId) $numberOfTimes : Number(this &gt;= 2) from accumulate( $r2 : SystemRebootEvent(this != $r1, computerId == $cId, this meets[1h] $r1), count($r2) ) $u : User(id == $uId, notified == false) then System.out.println($u.getUsername() + ", your computer was rebooted " + $numberOfTimes + " times after a reboot" ); modify($u) { setNotified(true); } usersNotified.push($u); // clear this in production end FUTURE OF CEP IN KOGITO As stated before, most of the CEP functionalities of Drools 7 will be supported in Drools 8 and in Kogito as well, albeit probably with some differences in the API. For Kogito specifically we’re planning to also provide some new features built on top that will allow users to leverage the power of the hybrid cloud to get high availability and some form of distributed processing by coordinating different rule units. These features are still in an Alpha stage, so if you’re interested please continue following this blog. Here’s a sneak peek of these features: In Kogito 1.12, bound to be released in the following days, we added the possibility to use a CloudEvents message via Kafka to trigger the execution of a DRL (see . This is currently supported only for stateless use cases but it is the first step to bring cloud-native replicated CEP Drools capabilities in Kogito. CONCLUSION We understood the idea behind having time based events in a rule engine and we saw a simple example of a monitoring service with the correct approach to testing CEP. We also saw how the monitoring service can be evolved using the same DRL syntax that we normally use to write business rules. For more information, please refer to the . The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/LS5Vilh35h0" height="1" width="1" alt=""/&gt;</content><dc:creator>Luca Molteni</dc:creator><feedburner:origLink>https://blog.kie.org/2021/10/event-driven-drools-cep-complex-event-processing-explained.html</feedburner:origLink></entry><entry><title type="html">MicroProfile Reactive Messaging 2.0 in WildFly 25</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/9Q4oDD5ymOQ/" /><author><name>Kabir Khan</name></author><id>https://wildfly.org//news/2021/10/14/MicroProfile-Reactive-Messaging-2.0-in-WildFly-25/</id><updated>2021-10-14T00:00:00Z</updated><content type="html">For WildFly 25, we upgraded the support from version 1 to 2. It contains a new annotation, which in conjunction with the new interface, were introduced in order to make it possible to push data into the MicroProfile Reactive streams from code initiated by a user. The MicroProfile Reactive Messaing implementation in WildFly is based on the project. The version included in WildFly 25, introduces a new to have more control over how we interact with Kafka. We expose this API in WildFly 25. This post will: * Take a simple web application, consisting of a few html pages, and add a Servlet filter to push information about page visits into Reactive Messaging via an Emitter. * These messages will be forwarded onto Kafka * Show a standalone application to read the last visited page per user from Kafka via the Kafka Streams API * Deploy the above application into WildFly, bundling the Kafka Streams API (which we don’t ship in WildFly) to read the last visited page per user. The code for the application is on . Additionally, you can find more information about the MicroProfile Reactive Messaging functionality in WildFly in the . RUNNING THE APPLICATION See the GitHub repository for instructions on how to build and run the different parts of the application. Here we will focus on explaining how it works. THE MAIN APPLICATION The main application is contained in the folder. The core of the application is a which link to each other. Now, we want to track which user visited which page. We do this by enhancing the application with a Servlet filter called : public class MessagingFilter extends HttpFilter { @Inject @Channel("from-filter") Emitter&lt;PageVisit&gt; messagingEmitter; FIrst we have field injected via CDI of type Emitter, called messagingEmitter, which is annotated with @Channel. This Emitter instance makes it a breeze to push data to the MicroProfile Reactive Messaging stream indicated by the value of the @Channel annotation (i.e. from-filter). @Override public void doFilter(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws IOException, ServletException { String user = getUsername(request); String address = request.getRemoteAddr(); String page = Paths.get(request.getRequestURI()).getFileName().toString(); PageVisit pv = new PageVisit(user, address, page); messagingEmitter.send(pv); Next we gather information about the request (user, address and page name), and bundle this information up in a bean. We then use the injected Emitter to send the PageVisit instance. The Emitter will then send the PageVisit to the from-filter stream. // Disable caching for the html pages ((HttpServletResponse)response).addHeader("Cache-control", "no-store"); ((HttpServletResponse)response).addHeader("Pragma", "no-cache"); filterChain.doFilter(request, response); } Getting the user name is simulated in the following method which randomly chooses a user for the current request. For the purposes of this demo this is to get a few different users in the data recorded when we click on the links when running the application. private String getUsername(HttpServletRequest servletRequest) { // Pretend we're looking up the authenticated user switch ((int)Math.round(Math.random() * 3)) { case 0: return "bob"; case 1: return "emma"; case 2: return "frank"; case 3: return "linda"; } return null; } } Next, we have an ApplicationScoped CDI bean called @ApplicationScoped public class MessagingBean { @Inject @Channel("special") Emitter&lt;PageVisit&gt; special; @Incoming("from-filter") @Outgoing("kafka-visits") public Message&lt;PageVisit&gt; fromFilter(PageVisit pageVisit) { if (pageVisit.getPage().equals("3.html")) { special.send(pageVisit); } Message&lt;PageVisit&gt; msg = Message.of(pageVisit); msg = KafkaMetadataUtil.writeOutgoingKafkaMetadata( msg, OutgoingKafkaRecordMetadata .&lt;String&gt;builder() .withKey(pageVisit.getUserName()) .build()); return msg; } @Incoming("special") public void special(PageVisit pageVisit) { System.out.println("===&gt; " + pageVisit.getUserName() + " visited " + pageVisit.getPage()); } } The fromFilter() method is annotated with the @Incoming("from-filter") annotation (from version 1 of the specification) and will receive all messages that were sent on our previous Emitter. Since the both the @Incoming and @Channel annotations use the value from-filter (i.e. they match), we end up with a simple in-memory stream. We could of course have routed this via Kafka, but for this example I wanted to keep the configuration needed to map to Kafka as simple as possible. The goes into more details about how to configure MicroProfile Reactive Messaging streams to consume from Kafka topics. The fromFilter() method is also annotated with @Outgoing("kafka-visits"), and so it is expected that all incoming messages from the from-filter stream will be forwarded onto the kafka-visits stream. The kafka-visits stream is backed by Kafka (we will see how to map this stream onto a Kafka topic in a second). In this case we decide that we want messages sent on this topic to have a Kafka key, so we: * Wrap the incoming PageVisit object in a Message object, which comes from the MicroProfile Reactive Messaging specification. * We then create an instance, where we set the key of the record to be the user. We add this metadata to the message by calling . The mentioned classes come from the new . * Finally we return the massaged Message containing our received PageVisit instance, which will forward it to the kafka-visits stream. Another thing going on in this example, is that we’re using an injected Emitter to 'fork' the sending of the received data to an additional location. In fromFilter(), if the page 3.html was visited, we will also send the received PageVisit via the injected Emitter. This in turn will send the PageVisit instance on the special stream indicated in its @Channel annotation. The special() method, annotated with @Incoming(`special) receives messages from the special stream (i.e. the ones sent via the Emitter). When running the application, and clicking on the 3 link, you should see output in the server logs. Additionally, every click on any link will show up in the Kafka consumer logs mentioned in the example . So, in addition to being able to easily send data from user-initiated code, Emitter is useful for 'forking' streams, so you can send data to more than one location. This functionality was not present in version 1 of the specification. To map the kafka-visits stream to a Kafka topic we do the configuration in : mp.messaging.connector.smallrye-kafka.bootstrap.servers=localhost:9092 mp.messaging.outgoing.kafka-visits.connector=smallrye-kafka mp.messaging.outgoing.kafka-visits.topic=page-visits mp.messaging.outgoing.kafka-visits.value.serializer=org.wildfly.blog.reactive.messaging.common.PageVisitsSerializer This points the mapping towards localhost:9092 to connect to Kafka, maps the kafka-visits stream to the page-visits kafka topic, and specifies to be used to serialize the PageVisit instances that we send to Kafka. The contains more detailed information about this configuration. If you deploy the application into WildFly, as outlined in the example , and you performed the optional step of connecting a Kafka consumer, you should see the output similar to this in the Kafka consumer terminal as you click the links in the application hosted at : frank 127.0.0.1app emma 127.0.0.13.html frank 127.0.0.11.html linda 127.0.0.13.html frank 127.0.0.11.html emma 127.0.0.12.html frank 127.0.0.13.html When you visit 3.html, there will be additional output from the special() method in WildFly’s server.log ===&gt; emma visited 3.html ===&gt; linda visited 3.html ===&gt; frank visited 3.html READING DATA FROM KAFKA IN A STANDALONE APPLICATION While it is nice to be able to send (and receive, although not shown in this example) messages via Kafka, we may want to query the data in Kafka later. The code for the command line application to query data from Kafka is contained in the folder. It contains a very simple (I am a beginner at this part) application to get the most recent page visits per user. It uses the API to interact with Kafka. The class calls through to a more interesting DataStoreWrapper class. public static void main(String[] args) throws Exception { try (DataStoreWrapper dsw = new DataStoreWrapper()) { dsw.init(); Map&lt;String, String&gt; lastPagesByUser = Collections.emptyMap(); try { dsw.readLastVisitedPageByUsers(); } catch (InvalidStateStoreException e) { } if (lastPagesByUser.size() == 0) { // It seems that although the stream is reported as RUNNING // in dsw.init() it still needs some time to settle. Until that // happens there is no data or we get InvalidStateStoreException Thread.sleep(4000); lastPagesByUser = dsw.readLastVisitedPageByUsers(); } System.out.println("Last pages visited:\n" + lastPagesByUser); } } } Note There is some error handling here. In case you get no entries, or if you get InvalidStateStoreException, try increasing the timeout in the sleep. Looking at the class, the first thing to note is that it is 'CDI ready'. Although this section will run it as a standalone application where CDI is not relevant, we will reuse this class later in an application deployed in WildFly. @ApplicationScoped public class DataStoreWrapper implements Closeable { private volatile KafkaStreams streams; We will initialise this streams instance in the init() method below. @Inject private ConfigSupplier configSupplier = new ConfigSupplier() { @Override public String getBootstrapServers() { return "localhost:9092"; } @Override public String getTopicName() { return "page-visits"; } }; The configSupplier field is inititalised to an implementation of which hard codes the values of the Kafka bootstrap servers, and the topic name. When deploying this into WildFly later we will use MicroProfile Config to set these values to avoid hard coding them. DataStoreWrapper() { } Next, we will take a look at the init() method where we set up the ability to query the stream. @PostConstruct void init() { try { Properties props = new Properties(); props.put(StreamsConfig.APPLICATION_ID_CONFIG, "streams-pipe"); props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, configSupplier.getBootstrapServers()); props.putIfAbsent(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, 0); props.putIfAbsent(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName()); props.putIfAbsent(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, PageVisitSerde.class.getName()); // For this we want to read all the data props.putIfAbsent(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); The above sets configuration properties to connect to kafka, and sets s for (de)serializing the Kafka record keys and values. The class is used to (de)serialise our class from earlier. We also specify that we want all the data stored on this topic. final StreamsBuilder builder = new StreamsBuilder(); KeyValueBytesStoreSupplier stateStore = Stores.inMemoryKeyValueStore("test-store"); KTable&lt;String, PageVisit&gt; source = builder.table( configSupplier.getTopicName(), Materialized.&lt;String, PageVisit&gt;as(stateStore) .withKeySerde(Serdes.String()) .withValueSerde(new PageVisitSerde())); final Topology topology = builder.build(); this.streams = new KafkaStreams(topology, props); Now we create a associated with the Kafka topic, and create a from that. In this case since we are using the Kafka record key (above we used the user for this when sending to Kafka) as the KTable key, we will get one entry (the latest) for each user. Note this is a very simple example, and not an in-depth exploration of the Kafka Streams API, so of course more advanced views on the stored data are possible! final CountDownLatch startLatch = new CountDownLatch(1); final AtomicReference&lt;KafkaStreams.State&gt; state = new AtomicReference&lt;&gt;(); streams.setStateListener((newState, oldState) -&gt; { state.set(newState); switch (newState) { case RUNNING: case ERROR: case PENDING_SHUTDOWN: startLatch.countDown(); } }); this.streams.start(); startLatch.await(10, TimeUnit.SECONDS); System.out.println("Stream started"); if (state.get() != KafkaStreams.State.RUNNING) { throw new IllegalStateException(); } Finally, we start the stream and wait for it to start. } catch (Exception e) { if (this.streams != null) { this.streams.close(); } throw new RuntimeException(e); } } The readLastVisitedPageByUsers() method uses the StateStore we set up earlier and returns all the found entries: public Map&lt;String, String&gt; readLastVisitedPageByUsers() { StoreQueryParameters&lt;ReadOnlyKeyValueStore&lt;String, PageVisit&gt;&gt; sqp = StoreQueryParameters.fromNameAndType("test-store", QueryableStoreTypes.keyValueStore()); final ReadOnlyKeyValueStore&lt;String, PageVisit&gt; store = this.streams.store(sqp); Map&lt;String, String&gt; lastPageByUser = new HashMap&lt;&gt;(); KeyValueIterator&lt;String, PageVisit&gt; it = store.all(); it.forEachRemaining(keyValue -&gt; lastPageByUser.put(keyValue.key, keyValue.value.getPage())); return lastPageByUser; } @PreDestroy public void close() { this.streams.close(); } } If you run the application, following the instructions in the example , you should see output like this: Stream started Last pages visited: {frank=3.html, emma=2.html, linda=3.html} As already mentioned, this will be the latest page visited for each user. READING DATA FROM KAFKA IN A WILDFLY APPLICATION WildFly does not ship with the Kafka Streams API, but we can still deploy the application above into WildFly with some adjustments in how we package it. The example contains more details, but in a nutshell we: * Include the Kafka Streams API jar in our deployment * Make sure we don’t include all the Kafka Streams API jar’s transitive dependencies in our deployment since they already exist in WildFly. * Modify the deployment’s META-INF/MANIFEST.MF to set up a dependency on the org.apache.kafka.client JBoss Module. This module contains the Kafka client jar, which is needed by the Kafka Streams API. In our standalone application, we hardcoded the bootstrap servers and the topic name. When deploying to WildFly we would like to avoid recompiling the application if, say, Kafka moves somewhere else, so we specify this information in : kafka.bootstrap.servers=localhost:9092 kafka.topic=page-visits We then create an implementation of the interface in . This is an ApplicationScoped CDI bean which gets injected with the MicroProfile Config containing the properties from the microprofile-config.properties file: @ApplicationScoped public class MpConfigConfigSupplier implements ConfigSupplier { @Inject Config config; @Override public String getBootstrapServers() { return config.getValue("kafka.bootstrap.servers", String.class); } @Override public String getTopicName() { return config.getValue("kafka.topic", String.class); } } Our class from earlier is a CDI bean, and so our MpConfigConfigSupplier will get injected into its configSupplier field, overwriting the default implementation that was used in the standalone application case: @ApplicationScoped public class DataStoreWrapper implements Closeable { private volatile KafkaStreams streams; @Inject private ConfigSupplier configSupplier = new ConfigSupplier() { // -- SNIP -- // This implementation gets replaced by the injected MpConfigConfigSupplier In order to be able to call this from a client, we add a simple : @Path("/") @Produces(MediaType.APPLICATION_JSON) public class StreamsEndpoint { @Inject DataStoreWrapper wrapper; @GET @Path("/last-visited") public Map&lt;String, String&gt; getLastVisited() { return wrapper.readLastVisitedPageByUsers(); } } This simply delegates to our DataStoreWrapper. If you deploy the application as outlined in the example , and visit you should see output like: {"frank":"3.html","emma":"2.html","linda":"3.html"} CONCLUSION We have seen how to leverage the new Emitter in MicroProfile Reactive Messaging 2 to push data to MicroProfile Reactive Messaging Streams, and how to send data to Kafka. We also used the new Kafka User API to set the Kafka record key in the data sent to Kafka. Although we did not receive data from Kafka in this example, we leveraged the Kafka Streams API to read the data we stored in Kafka in a standalone application as well as in an application deployed to WildFly. REFERENCES The contains more information on the various configuration options for using MicroProfile Reactive Messaging with Kafka in WildFly. Also, the contains a fuller reference of configuration options for Kafka, as well as more information about MicroProfile Reactive Messaging in general. Finally, the MicroProfile Reactive Messaging specification can be found in the GitHub project.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/9Q4oDD5ymOQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Kabir Khan</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/10/14/MicroProfile-Reactive-Messaging-2.0-in-WildFly-25/</feedburner:origLink></entry><entry><title type="html">JBang cheatsheet (2021)</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/NnBiAudJsh8/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/jbang-cheatsheet-2021/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=jbang-cheatsheet-2021</id><updated>2021-10-13T12:27:46Z</updated><content type="html">Here is my JBang cheatsheet which can help as a reference to code self-contained source-only Java programs with unmatched ease. Getting started with JBang Install JBang on Linux curl -Ls https://sh.jbang.dev | bash -s - app setup Create a JBang class  from a template jbang init --template=hello helloworld.java List of available JBang templates jbang template ... The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/NnBiAudJsh8" height="1" width="1" alt=""/&gt;</content><dc:creator>F.Marchioni</dc:creator><feedburner:origLink>http://www.mastertheboss.com/java/jbang-cheatsheet-2021/?utm_source=rss&amp;utm_medium=rss&amp;utm_campaign=jbang-cheatsheet-2021</feedburner:origLink></entry><entry><title>Printf-style debugging using GDB, Part 2</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ynVi4YgpSMk/printf-style-debugging-using-gdb-part-2" /><author><name>Kevin Buettner</name></author><id>8f0f522e-6890-42be-a93d-51f28d43bddf</id><updated>2021-10-13T07:00:00Z</updated><published>2021-10-13T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2021/10/05/printf-style-debugging-using-gdb-part-1"&gt;first article in this series&lt;/a&gt; introduced the &lt;a href="https://www.gnu.org/software/gdb/"&gt;GNU debugger, GDB&lt;/a&gt;, and in particular its &lt;code&gt;dprintf&lt;/code&gt; command, which displays variables from programs in a fashion similar to &lt;a href="https://developers.redhat.com/topics/c"&gt;C-language&lt;/a&gt; &lt;code&gt;printf&lt;/code&gt; statements. This article expands on the rich capabilities of printf-style debugging by showing how to save commands for reuse and how to save the output from the program and GDB for later examination.&lt;/p&gt; &lt;h2&gt;Listing currently defined breakpoints&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;dprintf&lt;/code&gt; command creates a special type of breakpoint. The &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Set-Breaks.html#index-info-breakpoints"&gt;&lt;code&gt;info breakpoints&lt;/code&gt; command&lt;/a&gt; displays all breakpoints; however, at the moment, we have only &lt;code&gt;dprintf&lt;/code&gt; breakpoints defined:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) info breakpoints Num Type Disp Enb Address What 1 dprintf keep y 0x0000000000401281 in insert at tree.c:41 breakpoint already hit 7 times printf "Allocating node for data=%s\n", data 2 dprintf keep y 0x00000000004012b9 in insert at tree.c:47 breakpoint already hit 6 times printf "Recursing left for %s at node %s\n", data, tree-&gt;data 3 dprintf keep y 0x00000000004012de in insert at tree.c:49 breakpoint already hit 6 times printf "Recursing right for %s at node %s\n", data, tree-&gt;data (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Saving dprintf commands for a later session&lt;/h2&gt; &lt;p&gt;In traditional printf-style debugging, print statements added to the program persist until they are removed. This is not the case when using the &lt;code&gt;dprintf&lt;/code&gt; command with GDB; both &lt;code&gt;dprintf&lt;/code&gt; breakpoints and ordinary breakpoints will persist throughout a GDB session, but they won't persist between sessions. However, breakpoints may be saved to a file for later reuse.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Save-Breakpoints.html#index-save-breakpoints"&gt;&lt;code&gt;save breakpoints&lt;/code&gt; command&lt;/a&gt; saves breakpoints to a file. The following example shows how to save breakpoints to a file named &lt;code&gt;my-dprintf-breakpoints&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) save breakpoints my-dprintf-breakpoints Saved to file 'my-dprintf-breakpoints'. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The resulting file consists of GDB breakpoint commands saved from the session. Thus, the file &lt;code&gt;my-dprintf-breakpoints&lt;/code&gt; contains three lines:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;dprintf /home/kev/ctests/tree.c:41,"Allocating node for data=%s\n", data dprintf /home/kev/ctests/tree.c:47,"Recursing left for %s at node %s\n", data, tree-&gt;data dprintf /home/kev/ctests/tree.c:49,"Recursing right for %s at node %s\n", data, tree-&gt;data &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If changes are made to the program in between GDB sessions, the line numbers specified by these commands may no longer be correct. If that happens, the most straightforward fix is to use a text editor to adjust them.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;my-dprintf-breakpoints&lt;/code&gt; file can be loaded into some future GDB session—by the programmer who saved them, or by another programmer debugging the same program—via the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Command-Files.html#index-source"&gt;&lt;code&gt;source&lt;/code&gt; command&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) quit $ gdb -q ./tree Reading symbols from ./tree... (gdb) source my-dprintf-breakpoints Dprintf 1 at 0x401281: file tree.c, line 41. Dprintf 2 at 0x4012b9: file tree.c, line 47. Dprintf 3 at 0x4012de: file tree.c, line 49. &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Redirecting output&lt;/h2&gt; &lt;p&gt;Printf-style debugging can generate a lot of output. It is often useful to send debugging output to a file for later analysis.&lt;/p&gt; &lt;p&gt;By default, output from a dynamic &lt;code&gt;printf&lt;/code&gt; is sent to GDB's console. Also, by default, the output from a program run under GDB is sent to the console, but via a different file descriptor. Therefore, output from GDB and the program are usually intermixed. But since different file descriptors are used, it's possible to redirect either GDB's output or program output to a file, or even both outputs to separate files.&lt;/p&gt; &lt;h3&gt;Logging GDB's output to a file&lt;/h3&gt; &lt;p&gt;GDB provides a number of commands for saving output from GDB to a file. I'll discuss a few of them here; see the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Logging-Output.html"&gt;GDB manual&lt;/a&gt; for more information.&lt;/p&gt; &lt;p&gt;Let's suppose that you wish to save a log of GDB output to a log file named &lt;code&gt;my-gdb-log&lt;/code&gt;. This is done by first issuing the command &lt;code&gt;set logging file my-gdb-log&lt;/code&gt;, followed by the command &lt;code&gt;set logging on&lt;/code&gt;. Later on, you can issue the &lt;code&gt;set logging off&lt;/code&gt; command to stop sending GDB output to the log file. Using the &lt;code&gt;dprintf&lt;/code&gt; commands established earlier, this is what the sequence of commands looks like:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) set logging file my-gdb-log (gdb) set logging on Copying output to my-gdb-log. Copying debug output to my-gdb-log. (gdb) run Starting program: /home/kev/ctests/tree Allocating node for data=dog ... scorpion wolf [Inferior 1 (process 321429) exited normally] (gdb) set logging off Done logging to my-gdb-log. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As shown in the example, both program output and GDB's output are still sent to the console. (The &lt;code&gt;set logging debugredirect on&lt;/code&gt; command can be used to send GDB's output only to the log file.) However, only GDB's output is placed in &lt;code&gt;my-gdb-log&lt;/code&gt;, as you can see by viewing that file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;Starting program: /home/kev/ctests/tree Allocating node for data=dog Recursing left for cat at node dog ... Recursing right for scorpion at node javelina Allocating node for data=scorpion [Inferior 1 (process 321429) exited normally] &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note, too, that no prompts or user-typed commands appear in the log output.&lt;/p&gt; &lt;h3&gt;Redirecting program output to a file&lt;/h3&gt; &lt;p&gt;The mechanism for redirecting program output to a file is simple; &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Input_002fOutput.html#index-redirection"&gt;the &gt; redirection operator is used with the &lt;code&gt;run&lt;/code&gt; command&lt;/a&gt; in much the same way that output is redirected by most shells. The example below shows how to run the program while redirecting program output to the file &lt;code&gt;my-program-output&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) run &gt;my-program-output Starting program: /home/kev/ctests/tree &gt;my-program-output Allocating node for data=dog ... Allocating node for data=scorpion [Inferior 1 (process 321813) exited normally] (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;my-program-output&lt;/code&gt; file now looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;cat coyote dog gecko javelina scorpion wolf cat coyote dog gecko javelina scorpion wolf&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Sending dprintf output to the same file as program output&lt;/h3&gt; &lt;p&gt;When saving program output to a file, you might want to place &lt;code&gt;dprintf&lt;/code&gt;-related output in the same file, intermixed with the rest of the program output. This can be done by making GDB invoke the program's &lt;code&gt;printf()&lt;/code&gt; function from the standard C library linked with the program. GDB's &lt;code&gt;dprintf-style&lt;/code&gt; setting is used to control where &lt;code&gt;dprintf&lt;/code&gt; related output is sent. The default &lt;code&gt;dprintf-style&lt;/code&gt; setting is &lt;code&gt;gdb&lt;/code&gt;; it causes GDB's internal &lt;code&gt;printf&lt;/code&gt; command to be used, sending output to the GDB console. When the &lt;code&gt;dprintf-style&lt;/code&gt; setting is &lt;code&gt;call&lt;/code&gt;, GDB will perform what is known as an &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Calling.html#index-inferior-functions_002c-calling"&gt;&lt;em&gt;inferior function call&lt;/em&gt;&lt;/a&gt;; i.e., it will call a function in the program being debugged, in this case &lt;code&gt;printf()&lt;/code&gt;. Therefore, the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Dynamic-Printf.html#index-dprintf_002dstyle-call"&gt;&lt;code&gt;set dprintf-style call&lt;/code&gt;&lt;/a&gt; command causes the output that is printed when hitting a &lt;code&gt;dprintf&lt;/code&gt; breakpoint to be performed by calling &lt;code&gt;printf()&lt;/code&gt; from within the program:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;(gdb) set dprintf-style call (gdb) run &gt;my-program-output Starting program: /home/kev/ctests/tree &gt;my-program-output [Inferior 1 (process 322195) exited normally] (gdb) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;my-program-output&lt;/code&gt; file now contains both &lt;code&gt;dprintf&lt;/code&gt; output and program output together:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-diff"&gt;Allocating node for data=dog Recursing left for cat at node dog ... scorpion wolf&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;GDB provides other commands that send &lt;code&gt;dprintf&lt;/code&gt; output to a different file descriptor, much like using &lt;code&gt;fprintf()&lt;/code&gt; instead of &lt;code&gt;printf()&lt;/code&gt;. These same facilities can also be used to invoke printf-style logging functions defined in the program. Refer to the &lt;a href="https://sourceware.org/gdb/current/onlinedocs/gdb/Dynamic-Printf.html#Dynamic-Printf"&gt;GDB manual&lt;/a&gt; for an explanation of these commands.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Look for the third and final article in this series, which shows powerful ways to interact with functions in your program from GDB, and how to automate the execution of GDB commands.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/13/printf-style-debugging-using-gdb-part-2" title="Printf-style debugging using GDB, Part 2"&gt;Printf-style debugging using GDB, Part 2&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ynVi4YgpSMk" height="1" width="1" alt=""/&gt;</summary><dc:creator>Kevin Buettner</dc:creator><dc:date>2021-10-13T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/13/printf-style-debugging-using-gdb-part-2</feedburner:origLink></entry><entry><title type="html">Infinispan 13.0.0.Final</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/nYsp4uRbOP0/infinispan-13-final" /><author><name>Tristan Tarrant</name></author><id>https://infinispan.org/blog/2021/10/12/infinispan-13-final</id><updated>2021-10-13T00:00:00Z</updated><content type="html">Triskaidekaphobia (/ˌtrɪskaɪˌdɛkəˈfoʊbiə/ TRIS-kye-DEK-ə-FOH-bee-ə, /ˌtrɪskə-/ TRIS-kə-; from Ancient Greek τρεισκαίδεκα (treiskaídeka) 'thirteen', and Ancient Greek φόβος (phóbos) 'fear') is fear or avoidance of the number 13. However, as you should know by now, the Infinispan team is fearless, and for this reason we are not afraid to proudly announce “Infinispan 13 (Triskaidekaphobia)”. In the grand tradition of our codenames, this one also happens to be the name of a beer () So, don’t be scared and let’s dive into all the new great things that are in this release. CORE * Polyglot configuration: XML, JSON and YAML can now be used interchangeably to configure every part of Infinispan. Here is an example of how to configure a cache with eviction and Protobuf encoding: &lt;distributed-cache&gt; &lt;encoding media-type="application/x-protostream"/&gt; &lt;memory max-size="1.5GB" when-full="REMOVE"/&gt; &lt;/distributed-cache&gt; { "distributed-cache" : { "encoding" : { "media-type" : "application/x-protostream" }, "memory" : { "max-size" : "1.5GB", "when-full" : "REMOVE" } } } distributedCache: encoding: mediaType: "application/x-protostream" memory: maxSize: "1.5GB" whenFull: "REMOVE" * Max-idle asynchronous touch: max-idle expiration makes reading an entry behave like a write: all owners must update the last access timestamp. With asynchronous touch the reader does not wait for their confirmation, and reading a max-idle entry is as fast as reading any other entry. * Metrics for cache size are now optional: calculating the accurate size of a cache is an expensive operation. Starting with this release, currentNumberOfEntries and currentNumberOfEntriesInMemory and totalNumberOfEntries will return -1 by default. You can re-enable accurate computation of these metrics if you really need them. We will be adding high-performance estimates for these metrics in a future release. QUERY * Delete by query: Ickle (Infinispan’s query language) now supports DELETE FROM queries using all of the supported clauses, both for indexed and non-indexed caches: query.create("DELETE FROM books WHERE page_count &gt; 500").executeStatement(); PERSISTENCE * The Soft-index file store is now our default persistent file-based cache store. Compared to the old single-file store, it no longer needs to store all keys in memory, plus it supports persistent memory via the awesome library. Old file stores will be automatically migrated on first use. * SQL cache store which maps database tables and queries to Protobuf, greatly simplifying accessing your existing data. For example, you can expose a single table books: &lt;table-jdbc-store table-name="books"&gt; &lt;schema message-name="books_value" key-message-name="books_key" package="library" embedded-key="true"/&gt; &lt;/table-jdbc-store&gt; or use your own queries &lt;query-jdbc-store&gt; &lt;queries key-columns="isbn"&gt; &lt;select-single&gt;SELECT isbn, title FROM books WHERE isbn = :isbn&lt;/select-single&gt; &lt;select-all&gt;SELECT isbn, title FROM books&lt;/select-all&gt; &lt;delete&gt;DELETE FROM books WHERE isbn = :key&lt;/delete&gt; &lt;delete-all&gt;DELETE FROM books&lt;/delete-all&gt; &lt;upsert&gt;INSERT INTO books (isbn, title) VALUES (:key, :value) ON CONFLICT (isbn) DO UPDATE SET title = :value&lt;/upsert&gt; &lt;size&gt;SELECT COUNT(*) FROM books&lt;/size&gt; &lt;/queries&gt; &lt;schema message-name="books_value" key-message-name="books_key" package="library" embedded-key="true"/&gt; &lt;/query-jdbc-store&gt; SERVER * Configuration overlays: you can specify multiple configuration files, in any of the supported formats, and they will be combined to form the final configuration. You can even mix formats: server.sh -c base.xml -c layer.yml -c custom.json * Mutable cache configuration: you can now update cache configurations cluster-wide at runtime with the CLI. The following example changes the maximum number of entries of a cache: alter cache mycache --attribute=memory.max-count --value=10000 * Thread Pool Consolidation: The prior Infinispan non blocking thread pool has been consolidated with the Netty event loop reducing the number of threads required in the server. * REST listeners: It is now possible to listen to cache events over HTTP using Server-Sent Events. curl --digest -u user:password -N http://127.0.0.1:11222/rest/v2/caches/mycache?action=listen event: cache-entry-created data: data: { data: "_type": "string", data: "_value": "k1" data: } event: cache-entry-modified data: data: { data: "_type": "string", data: "_value": "k1" data: } event: cache-entry-removed data: data: { data: "_type": "string", data: "_value": "k1" data: } * Rebalancing management: control cluster re-balancing from the REST API, CLI, and Console. * Simple TLS for clustering: Infinispan Server can automatically enable TLS for the cluster transport simply by specifying a security realm with a keystore/truststore server identity: &lt;cache-container name="default" statistics="true"&gt; &lt;transport cluster="cluster" server:security-realm="cluster"/&gt; &lt;/cache-container&gt; * Distributed Security Realm: a server security realm which can aggregate multiple sub-realms, trying each one in turn. For example, you can use this to support both certificate and password authentication on the same server. * PEM key/trust stores: Support for PEM files for both keys and certificates without the need to convert them to Java keystores first. * Full support for TLSv1.3 via native OpenSSL. CLUSTER MIGRATION We’ve done quite a lot of work on the cluster igration operations, making the process smoother from the REST API, CLI, and with our Kubernetes Operator. * Manually changing configurations of the cache(s) is no longer necessary * New methods in the REST API to control the migration * Caches created dynamically are now supported * Simplified configuration CROSS-SITE REPLICATION * Improve cross-site replication observability * The cross-site view (sorted list of site names currently online) and relay-nodes (members who are responsible for relaying messages between sites) are now exposed via CLI/REST/JMX. * Detailed metrics exposed per site and per cache (response times, number of messages) * Improve some error messages with more details. INFINISPAN CONSOLE * Encoding-aware entry editing * Rebalancing operations per-cache and per-cluster CLOUD * Helm charts: create Infinispan clusters with a Helm chart that lets you specify values for build and deployment configuration. Server configuration is declared using Yaml in .Values. This allows the server configuration to be customized entirely without having to update helm-chart templates locally. * Operator: many fixes and improvements: * Migrated operator-sdk from v0.18.0 → v1.3.2 * Migrated packagemanifest → OLM bundle format * K8s 1.22 deprecated APIs removed KUBERNETES CLI * Easily connect a CLI to an operator-managed Infinispan cluster without having to specify connection details: kubectl infinispan shell -n default mycluster [mycluster-0-37266@mycluster//containers/default]&gt; TESTING * An InfinispanContainer which makes it easy to test your applications via the awesome library try (InfinispanContainer container = new InfinispanContainer()) { container.start(); try (RemoteCacheManager cacheManager = container.getRemoteCacheManager()) { RemoteCache&lt;Object, Object&gt; testCache = cacheManager.administration().getOrCreateCache("test", DefaultTemplate.DIST_SYNC); testCache.put("key", "value"); assertEquals("value", testCache.get("key")); } } CLUSTERED COUNTERS * Strong counters can now expire (experimental). The counter value is reset to its initial value which may be useful to code a cluster-wide rate limiter. OTHER * Works with JDK 17 (and still works with JDK 8 and JDK 11) * Lots of bug fixes DOCUMENTATION AND TUTORIALS * Updated cache configuration docs with tabbed examples with JSON and YAML. * Added new guides for indexing and querying caches, Hot Rod clients, and Helm charts. * Re-organized Infinispan simple tutorials as part of the ongoing effort to clearly separate remote caches from embedded caches in our content. Infinispan simple tutorials now have their own documentation on our site at: * Updated documentation for configuring persistent cache stores and JVM memory management, including revisions to improve style, grammar, and provide high-level scanning and readability. * Replaced the Integration Guide with a dedicated guide for Spring users as well as a guide for Hibernate caches. We’ve also linked to community projects with Quarkus, Vert.x, Keycloak, Camel, and WildFly. Check out the new Integrations category on the docs home page at: Be sure to read through before getting started with lucky 13.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/nYsp4uRbOP0" height="1" width="1" alt=""/&gt;</content><dc:creator>Tristan Tarrant</dc:creator><feedburner:origLink>https://infinispan.org/blog/2021/10/12/infinispan-13-final</feedburner:origLink></entry><entry><title>Tools and practices for remote development teams</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/YKSf73GCfog/tools-and-practices-remote-development-teams" /><author><name>Jason Dudash, Andy Krohg</name></author><id>9642af13-ef3a-40d8-86ab-f12a83aacb4e</id><updated>2021-10-12T07:00:00Z</updated><published>2021-10-12T07:00:00Z</published><summary type="html">&lt;p&gt;During the height of the COVID-19 global pandemic, tens of millions of workers transitioned from the office to working from home. It was an unfamiliar way of doing things for many organizations—a true sink-or-swim scenario. Development teams are among those affected, and the challenges that we face are sometimes very specific. In this article, we explore a few tools and practices that can help distributed development teams work and collaborate from home. Hopefully, this exploration will be helpful to you and your team seeking a “new normal" after COVID-19.&lt;/p&gt; &lt;h2&gt;Accessing the development environment&lt;/h2&gt; &lt;p&gt;Perhaps the most poignant of remote development challenges is not having access to a physical workstation. When developers work together in a company office, it is straightforward to issue everyone a desktop PC that’s equipped with sufficient firepower for the job at hand. Without this convenience, however, many organizations fall short of offering an optimal experience. Let's consider a few common approaches.&lt;/p&gt; &lt;h3&gt;Laptops to the rescue?&lt;/h3&gt; &lt;p&gt;Company laptops are a standard issue for any full-time job, but developers represent a special case. The &lt;a href="https://developers.redhat.com/blog/2020/06/16/enterprise-kubernetes-development-with-odo-the-cli-tool-for-developers"&gt;inner loop of development&lt;/a&gt; demands a plethora of builds, tests, and local deployments. This demanding cycle renders the compute resources of a typical work laptop insufficient. The beefier machines sometimes given to developers to mitigate this are still a compromise between &lt;em&gt;user experience&lt;/em&gt; and &lt;em&gt;risk&lt;/em&gt;. Portable workstations are more likely to be damaged, lost, or stolen, presenting cogent risks of not only financial impact but the loss of data privacy. With physical access to a device that houses classified information, a malicious actor could create problems that eclipse concerns related to infrastructure costs.&lt;/p&gt; &lt;h3&gt;Okay, well then VMs?&lt;/h3&gt; &lt;p&gt;Another common practice is for development teams to use virtual machines in a data center for their development environment. The advantage here is that the laptops need not be anything special; the laptops only serve as a means to access the VM. The problem is network latency. Remote desktop environments aren’t as smooth or performant as I/O, especially with insufficient network bandwidth. Developers who are working from home won’t always have optimal network speed to work with, whether it’s unavailable where they live or because they’re competing with other household members for bandwidth. In the latter case, it’s not always as simple as kicking the kids off Netflix; remote developers may live with children using remote learning software or other adults conducting business remotely.&lt;/p&gt; &lt;h3&gt;What about an in-browser IDE?&lt;/h3&gt; &lt;p&gt;The advent of containers has utterly transformed the landscape for application workloads over the last several years. The dazzling metropolis of communities that have formed around the technology has advanced the maturity and consumability of containers for virtually all aspects of IT, including &lt;a href="https://developers.redhat.com/topics/containers"&gt;containerized development environments&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;True masters of remote development can serve up workspaces through an in-browser IDE like &lt;a href="https://developers.redhat.com/products/codeready-workspaces/overview"&gt;CodeReady Workspaces&lt;/a&gt;. Centralizing workspace management to an underlying platform like &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; results in workspaces that are powerful and tool-ready enough that all you need on your laptop is a modern browser. This strategy offers better performance over virtual machines because the connection is powered by a web socket, which has lower bandwidth requirements than a remote desktop client. It’s also snappier in that the containers’ shared kernel enables faster start times than virtual machines. As if that's not enough, just think &lt;a href="https://developers.redhat.com/blog/2019/02/18/containerized-python-flask-development-environment-red-hat-codeready-workspaces"&gt;how much easier onboarding a new teammate will be&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you’re curious about the in-browser IDE option, you can try it out yourself using the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, a free shared OpenShift cluster with CodeReady Workspaces preconfigured. You can get started experimenting with it in as little as two minutes.&lt;/p&gt; &lt;h2&gt;Pair programming for remote teams&lt;/h2&gt; &lt;p&gt;The lack of in-person interaction is another remote workplace challenge that uniquely impacts developers. When everyone is working from home, having a colleague look over your shoulder at something you’re working on is more of an inventive metaphor than a literal practice. Sure, there are plenty of video conferencing apps, but they generally don’t deliver a quick and simple pair programming experience.&lt;/p&gt; &lt;p&gt;This is another area where CodeReady Workspaces excels, as it can inherit role-based access controls (RBAC) from OpenShift, permitting a developer to grant read or write access to collaborators in workspaces they own. With OpenShift’s software-defined networking, collaboratively fixing a bug is as easy as generating a link to a workspace, or to a locally-deployed app, and sharing it with others. As more companies embrace an increasingly remote workforce, preserving the rituals of &lt;a href="https://en.wikipedia.org/wiki/Rubber_duck_debugging"&gt;rubber duck debugging&lt;/a&gt; and pair programming are of the utmost importance for long-term success.&lt;/p&gt; &lt;h2&gt;Automated pipelines for CI/CD handoffs&lt;/h2&gt; &lt;p&gt;Working from home deprives workers of all teams from a great many easily observable things, like chance encounters with Greg at the Keurig or quarreling with coworkers over who has to sit in the squeaky swivel chair. But remote work also disrupts something a bit less obvious: &lt;em&gt;continuity&lt;/em&gt;. Workers are prone to more tiny distractions and sequence breaks at home, as it presents a much different atmosphere than an office setting. This doesn’t necessarily reduce how much work people get done, but it makes achieving uninterrupted workstreams more challenging.&lt;/p&gt; &lt;p&gt;The impact this has on developers is the potential to introduce slowdowns into handoffs related to continuous integration and continuous deployment (&lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt;). When colleagues on ops teams and other stakeholder groups aren’t monitoring incoming workloads contiguously, there’s a greater chance developers will have to wait longer for their code to be deployed. For this reason, it’s more important than ever before to afford developers with self-service capabilities and &lt;a href="https://developers.redhat.com/blog/2021/01/13/getting-started-with-tekton-and-pipelines?ts=1633456468158"&gt;automated pipelines&lt;/a&gt; so that teams can keep moving.&lt;/p&gt; &lt;p&gt;Pipelines turn the team build, test, verification, and release process into consistent and repeatable automation. By hooking up pipelines into source control repositories and into team chat, developers and operations (&lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt;) can come together in a way that removes barriers and creates a shared awareness with a recorded history. While these benefits directly address the lament of continuity breaks in today’s IT landscape, they’re guaranteed to deliver improved lead times and developer experience regardless of how pandemic conditions unfold in the future.&lt;/p&gt; &lt;h2&gt;Working remotely is the new normal&lt;/h2&gt; &lt;p&gt;It’s widely expected that working remotely will become common for at least some of the workweek (almost no one wants that everyday commute again). If you fast-forward to a year from now, we expect to see successful development teams being the ones that embraced the shift. Those teams will be doing the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Providing shared IDEs to facilitate collaboration.&lt;/li&gt; &lt;li&gt;Establishing a repository of self-service and single-click workspaces for IDE consistency.&lt;/li&gt; &lt;li&gt;Triggering &lt;a href="https://www.redhat.com/en/topics/devops/what-is-ci-cd#ci/cd-tools"&gt;CI/CD pipelines&lt;/a&gt; off of source control to automate away manual operations tasks.&lt;/li&gt; &lt;li&gt;Providing dashboards and ChatOps feedback from the CI/CD for shared awareness.&lt;/li&gt; &lt;li&gt;Leveraging new tooling so "&lt;a href="https://containerjournal.com/topics/container-security/what-will-it-take-to-shift-kubernetes-security-left/"&gt;shifting left on security&lt;/a&gt;" doesn’t burden developers.&lt;/li&gt; &lt;li&gt;Leveraging code quality tools within their CI/CD pipeline to minimize the effort of manual code reviews.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;As your team evolves into remote capable operating models, keeping the status-quo of a local-only development process just won’t cut it. Those environments are a source of friction that will bottleneck your efforts and easily get you off schedule and over cost. Adopting new team culture, defining new ways to collaborate, and bringing in new shared development tools will help you meet the evolution into remote work in a way that becomes your “normal."&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Find out more about CodeReady Workspaces by watching this &lt;a href="https://www.youtube.com/watch?v=fAjx3bXcBZI"&gt;Developer Experience Office Hours video&lt;/a&gt; dedicated to CodeReady Workspaces and Eclipse Che. You can also learn about best practices for developer workspaces in this &lt;a href="https://go.govloop.com/Modern-Developer-Workspaces.html"&gt;short report&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Want more insights like these? Follow Andy on GitHub, &lt;a href="https://github.com/andykrohg"&gt;@andykrohg&lt;/a&gt;. Follow Dudash on Twitter (&lt;a href="https://twitter.com/dudashtweets"&gt;@dudashtweets&lt;/a&gt;) or GitHub (&lt;a href="https://github.com/dudash"&gt;@dudash&lt;/a&gt;).&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/12/tools-and-practices-remote-development-teams" title="Tools and practices for remote development teams"&gt;Tools and practices for remote development teams&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/YKSf73GCfog" height="1" width="1" alt=""/&gt;</summary><dc:creator>Jason Dudash, Andy Krohg</dc:creator><dc:date>2021-10-12T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/10/12/tools-and-practices-remote-development-teams</feedburner:origLink></entry><entry><title type="html">WildFly Bootable JAR 6.0 is released!</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/KkHe4krHf40/" /><author><name>Jean-François Denise</name></author><id>https://wildfly.org//news/2021/10/12/bootable-jar-6.0.Final-Released/</id><updated>2021-10-12T00:00:00Z</updated><content type="html">The 6.0.0.Final version of the has been released. For people who are not familiar with the WildFly Bootable JAR, I strongly recommend that you read this that covers it in detail. NEW EXAMPLES TO HIGHLIGHT NEW GALLEON LAYERS INTRODUCED IN WILDFLY 25 For each new release of the WildFly Bootable JAR Maven plugin we are upgrading the dependency of the to the latest WildFly release and highlight new features that you can use when building WildFly Bootable JARs. For 6.0.0.Final, we have added two new examples to cover some new Galleon layers that are of particualr importance for Wildfly Bootable JAR packaging. AUTOMATIC GENERATION OF SELF SIGNED CERTIFICATE Up to now, in order to configure HTTPS when using elytron, we had to include a keystore in the Bootable JAR during packaging. During development phases we generally don’t really care of the content of the keystore (and generating one implies extra steps… ). WildFly 25 introduces the undertow-https Galleon layer that adds an elytron secured https-listener that generates a self signed certificate on first connection. The has been evolved with the self-signed profile to provision a server that makes use of the undertow-https Galleon layer. OPENID CONNECT (OIDC) NATIVE SUPPORT Up to now, as highlighted in the , in order to secure a deployment using we had to provision both WildFly Galleon feature-pack and Keycloak OIDC adapter feature-pack. Starting with WildFly 25, we are now relying on the WildFly native OIDC support and are deprecating the use of the Keycloack Galleon feature-pack. WildFly 25 introduces the elytron-oidc-client Galleon layer to provision the elytron-oidc-client subsystem that allows to interact with OIDC compliant authorization servers (such as the server). This native support is very similar to the Keycloack one. Migrating from using Keycloak Galleon adapter to using WildFly OIDC native support when building a Bootable JAR is a matter of: * Removing the Keycloak OIDC adapter feature-pack (org.keycloak:keycloak-adapter-galleon-pack) from the plugin configuration. * Replacing the keycloak-client-oidc Galleon layer by the elytron-oidc-client Galleon layer in the plugin configuration. * Change the &lt;auth-method&gt;KEYCLOAK&lt;/auth-method&gt; to &lt;auth-method&gt;OIDC&lt;/auth-method&gt; in your web.xml file. * If you were using WEB-INF/keycloak.json file you would need to rename it to WEB-INF/oidc.json and update its content by following the . * If you were calling WildFly CLI scripts to update the subsystem configuration, you will need to adjust the management operations. More information can be found in the . The covers the steps required to use this new Galleon layer. TO CONCLUDE Finally we would really appreciate if if you would keep us posted with your feedback and new requirements. (You can log these as new .) This will help us evolve the WildFly Bootable JAR experience in the right direction. Thank-you! JF Denise&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/KkHe4krHf40" height="1" width="1" alt=""/&gt;</content><dc:creator>Jean-François Denise</dc:creator><feedburner:origLink>https://wildfly.org//news/2021/10/12/bootable-jar-6.0.Final-Released/</feedburner:origLink></entry></feed>
